
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/long_train_hpc/step_1300", "results": {"raw": {"test": [{"em": 19.287374128582496, "f1": 23.310688886657374}, {"em": 15.348837209302326, "f1": 18.35528022961964}, {"em": 19.629057187017, "f1": 24.18559277756193}]}, "total": {"test_em": 18.088422841633943, "test_em_se": 2.6917453626709467, "test_f1": 21.950520631279648, "test_f1_se": 3.5579406892314904}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/long_train_hpc/step_13000", "results": {"raw": {"test": [{"em": 0.23237800154918667, "f1": 3.0507889903032464}, {"em": 0.31007751937984496, "f1": 3.8169367256605007}, {"em": 0.0, "f1": 3.6837017158891685}]}, "total": {"test_em": 0.18081850697634386, "test_em_se": 0.1825741249035604, "test_f1": 3.5171424772843047, "test_f1_se": 0.46320191562369495}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/long_train_hpc/step_2600", "results": {"raw": {"test": [{"em": 8.752904725019365, "f1": 14.094850916694357}, {"em": 7.984496124031008, "f1": 13.754619372007094}, {"em": 8.268933539412673, "f1": 13.391789355184054}]}, "total": {"test_em": 8.335444796154349, "test_em_se": 0.43962695864607526, "test_f1": 13.747086547961835, "test_f1_se": 0.39786301843722016}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_7800", "results": {"raw": {"test": [{"em": 0.0, "f1": 3.5089118144061517}, {"em": 0.07751937984496124, "f1": 5.016029942530271}, {"em": 0.0, "f1": 3.8309141574364607}]}, "total": {"test_em": 0.025839793281653745, "test_em_se": 0.050645994832041345, "test_f1": 4.118618638124294, "test_f1_se": 0.8981361220395749}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_7800", "results": {"raw": {"test": [{"em": 0.0, "f1": 3.5089118144061517}, {"em": 0.07751937984496124, "f1": 5.016029942530271}, {"em": 0.0, "f1": 3.8309141574364607}]}, "total": {"test_em": 0.025839793281653745, "test_em_se": 0.050645994832041345, "test_f1": 4.118618638124294, "test_f1_se": 0.8981361220395749}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_7800", "results": {"raw": {"test": [{"em": 0.0, "f1": 3.5089118144061517}]}, "total": {"test_em": 0.0, "test_em_se": NaN, "test_f1": 3.5089118144061517, "test_f1_se": NaN}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_1300", "results": {"raw": {"test": [{"em": 4.415182029434547, "f1": 11.59790464343451}, {"em": 0.6976744186046512, "f1": 8.141858680746845}, {"em": 1.777434312210201, "f1": 8.371385590911304}]}, "total": {"test_em": 2.2967635867497997, "test_em_se": 2.164075187422901, "test_f1": 9.370382971697552, "test_f1_se": 2.186830785497224}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_2600", "results": {"raw": {"test": [{"em": 16.886134779240898, "f1": 23.050568049720535}, {"em": 18.6046511627907, "f1": 24.124974766146053}, {"em": 18.39258114374034, "f1": 24.34530463287618}]}, "total": {"test_em": 17.96112236192398, "test_em_se": 1.0602991032595825, "test_f1": 23.840282482914258, "test_f1_se": 0.783896257718189}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_1300", "results": {"raw": {"test": [{"em": 4.415182029434547, "f1": 11.59790464343451}, {"em": 0.6976744186046512, "f1": 8.141858680746845}, {"em": 1.777434312210201, "f1": 8.371385590911304}]}, "total": {"test_em": 2.2967635867497997, "test_em_se": 2.164075187422901, "test_f1": 9.370382971697552, "test_f1_se": 2.186830785497224}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_2600", "results": {"raw": {"test": [{"em": 16.886134779240898, "f1": 23.050568049720535}, {"em": 18.6046511627907, "f1": 24.124974766146053}, {"em": 18.39258114374034, "f1": 24.34530463287618}]}, "total": {"test_em": 17.96112236192398, "test_em_se": 1.0602991032595825, "test_f1": 23.840282482914258, "test_f1_se": 0.783896257718189}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_3900", "results": {"raw": {"test": [{"em": 0.6196746707978311, "f1": 6.430495892538802}, {"em": 1.627906976744186, "f1": 7.598106965741532}, {"em": 0.3091190108191654, "f1": 5.279799336200877}]}, "total": {"test_em": 0.8522335527870609, "test_em_se": 0.7802040495688798, "test_f1": 6.43613406482707, "test_f1_se": 1.311717660819032}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_1300", "results": {"raw": {"test": [{"em": 4.415182029434547, "f1": 11.59790464343451}, {"em": 0.6976744186046512, "f1": 8.141858680746845}, {"em": 1.777434312210201, "f1": 8.371385590911304}]}, "total": {"test_em": 2.2967635867497997, "test_em_se": 2.164075187422901, "test_f1": 9.370382971697552, "test_f1_se": 2.186830785497224}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_2600", "results": {"raw": {"test": [{"em": 16.886134779240898, "f1": 23.050568049720535}, {"em": 18.6046511627907, "f1": 24.124974766146053}, {"em": 18.39258114374034, "f1": 24.34530463287618}]}, "total": {"test_em": 17.96112236192398, "test_em_se": 1.0602991032595825, "test_f1": 23.840282482914258, "test_f1_se": 0.783896257718189}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_5200", "results": {"raw": {"test": [{"em": 0.69713400464756, "f1": 5.25658538239424}, {"em": 1.317829457364341, "f1": 7.618729364054122}, {"em": 1.1591962905718702, "f1": 6.719507033911071}]}, "total": {"test_em": 1.058053250861257, "test_em_se": 0.3649113435551277, "test_f1": 6.53160726011981, "test_f1_se": 1.3491344998093988}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_6500", "results": {"raw": {"test": [{"em": 0.0, "f1": 3.601785383542276}, {"em": 0.07751937984496124, "f1": 4.190543110863382}, {"em": 0.0, "f1": 3.675359378038974}]}, "total": {"test_em": 0.025839793281653745, "test_em_se": 0.050645994832041345, "test_f1": 3.8225626241482114, "test_f1_se": 0.3630156210394486}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_9100", "results": {"raw": {"test": [{"em": 0.0, "f1": 2.277752180741008}, {"em": 0.15503875968992248, "f1": 2.804295814063316}, {"em": 0.0, "f1": 2.6451745285122095}]}, "total": {"test_em": 0.05167958656330749, "test_em_se": 0.10129198966408269, "test_f1": 2.5757408411055107, "test_f1_se": 0.3055920705837912}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_10400", "results": {"raw": {"test": [{"em": 1.0844306738962044, "f1": 4.667945255421082}, {"em": 1.0852713178294573, "f1": 5.500937958250401}, {"em": 0.46367851622874806, "f1": 3.7421323430646067}]}, "total": {"test_em": 0.8777935026514698, "test_em_se": 0.40583296542053227, "test_f1": 4.637005185578697, "test_f1_se": 0.9955997859190142}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_11700", "results": {"raw": {"test": [{"em": 0.15491866769945778, "f1": 3.9328589066347748}, {"em": 0.07751937984496124, "f1": 3.833857998675171}, {"em": 0.0, "f1": 4.288739385406335}]}, "total": {"test_em": 0.077479349181473, "test_em_se": 0.08765349658767232, "test_f1": 4.01848543023876, "test_f1_se": 0.270707612573117}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_13000", "results": {"raw": {"test": [{"em": 0.07745933384972889, "f1": 4.100454225419051}, {"em": 0.0, "f1": 4.418886104499059}, {"em": 0.0, "f1": 3.9149160572332953}]}, "total": {"test_em": 0.02581977794990963, "test_em_se": 0.05060676478182288, "test_f1": 4.144752129050468, "test_f1_se": 0.2884335634505966}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/final_model", "results": {"raw": {"test": [{"em": 0.07745933384972889, "f1": 3.8713917114319876}, {"em": 0.07751937984496124, "f1": 4.061850098953285}, {"em": 0.07727975270479134, "f1": 3.942117766621321}]}, "total": {"test_em": 0.07741948879982716, "test_em_se": 0.00014109283222840812, "test_f1": 3.958453192335531, "test_f1_se": 0.10894458579210015}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_1300", "results": {"raw": {"test": [{"em": 4.415182029434547, "f1": 11.59790464343451}, {"em": 0.6976744186046512, "f1": 8.141858680746845}, {"em": 1.777434312210201, "f1": 8.371385590911304}]}, "total": {"test_em": 2.2967635867497997, "test_em_se": 2.164075187422901, "test_f1": 9.370382971697552, "test_f1_se": 2.186830785497224}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_2600", "results": {"raw": {"test": [{"em": 16.886134779240898, "f1": 23.050568049720535}, {"em": 18.6046511627907, "f1": 24.124974766146053}, {"em": 18.39258114374034, "f1": 24.34530463287618}]}, "total": {"test_em": 17.96112236192398, "test_em_se": 1.0602991032595825, "test_f1": 23.840282482914258, "test_f1_se": 0.783896257718189}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_1300", "results": {"raw": {"test": [{"em": 4.415182029434547, "f1": 11.59790464343451}, {"em": 0.6976744186046512, "f1": 8.141858680746845}, {"em": 1.777434312210201, "f1": 8.371385590911304}]}, "total": {"test_em": 2.2967635867497997, "test_em_se": 2.164075187422901, "test_f1": 9.370382971697552, "test_f1_se": 2.186830785497224}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_2600", "results": {"raw": {"test": [{"em": 16.886134779240898, "f1": 23.050568049720535}, {"em": 18.6046511627907, "f1": 24.124974766146053}, {"em": 18.39258114374034, "f1": 24.34530463287618}]}, "total": {"test_em": 17.96112236192398, "test_em_se": 1.0602991032595825, "test_f1": 23.840282482914258, "test_f1_se": 0.783896257718189}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_3900", "results": {"raw": {"test": [{"em": 0.6196746707978311, "f1": 6.430495892538801}, {"em": 1.627906976744186, "f1": 7.598106965741526}, {"em": 0.3091190108191654, "f1": 5.279799336200871}]}, "total": {"test_em": 0.8522335527870609, "test_em_se": 0.7802040495688798, "test_f1": 6.4361340648270655, "test_f1_se": 1.3117176608190322}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_5200", "results": {"raw": {"test": [{"em": 0.69713400464756, "f1": 5.25658538239424}, {"em": 1.317829457364341, "f1": 7.618729364054122}, {"em": 1.1591962905718702, "f1": 6.719507033911071}]}, "total": {"test_em": 1.058053250861257, "test_em_se": 0.3649113435551277, "test_f1": 6.53160726011981, "test_f1_se": 1.3491344998093988}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_6500", "results": {"raw": {"test": [{"em": 0.0, "f1": 3.601785383542277}, {"em": 0.07751937984496124, "f1": 4.190543110863386}, {"em": 0.0, "f1": 3.675359378038974}]}, "total": {"test_em": 0.025839793281653745, "test_em_se": 0.050645994832041345, "test_f1": 3.8225626241482122, "test_f1_se": 0.36301562103945056}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}