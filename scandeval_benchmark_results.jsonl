
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/long_train_hpc/step_1300", "results": {"raw": {"test": [{"em": 19.287374128582496, "f1": 23.310688886657374}, {"em": 15.348837209302326, "f1": 18.35528022961964}, {"em": 19.629057187017, "f1": 24.18559277756193}]}, "total": {"test_em": 18.088422841633943, "test_em_se": 2.6917453626709467, "test_f1": 21.950520631279648, "test_f1_se": 3.5579406892314904}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/long_train_hpc/step_13000", "results": {"raw": {"test": [{"em": 0.23237800154918667, "f1": 3.0507889903032464}, {"em": 0.31007751937984496, "f1": 3.8169367256605007}, {"em": 0.0, "f1": 3.6837017158891685}]}, "total": {"test_em": 0.18081850697634386, "test_em_se": 0.1825741249035604, "test_f1": 3.5171424772843047, "test_f1_se": 0.46320191562369495}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/long_train_hpc/step_2600", "results": {"raw": {"test": [{"em": 8.752904725019365, "f1": 14.094850916694357}, {"em": 7.984496124031008, "f1": 13.754619372007094}, {"em": 8.268933539412673, "f1": 13.391789355184054}]}, "total": {"test_em": 8.335444796154349, "test_em_se": 0.43962695864607526, "test_f1": 13.747086547961835, "test_f1_se": 0.39786301843722016}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}