
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/long_train_hpc/step_1300", "results": {"raw": {"test": [{"em": 19.287374128582496, "f1": 23.310688886657374}, {"em": 15.348837209302326, "f1": 18.35528022961964}, {"em": 19.629057187017, "f1": 24.18559277756193}]}, "total": {"test_em": 18.088422841633943, "test_em_se": 2.6917453626709467, "test_f1": 21.950520631279648, "test_f1_se": 3.5579406892314904}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/long_train_hpc/step_13000", "results": {"raw": {"test": [{"em": 0.23237800154918667, "f1": 3.0507889903032464}, {"em": 0.31007751937984496, "f1": 3.8169367256605007}, {"em": 0.0, "f1": 3.6837017158891685}]}, "total": {"test_em": 0.18081850697634386, "test_em_se": 0.1825741249035604, "test_f1": 3.5171424772843047, "test_f1_se": 0.46320191562369495}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/long_train_hpc/step_2600", "results": {"raw": {"test": [{"em": 8.752904725019365, "f1": 14.094850916694357}, {"em": 7.984496124031008, "f1": 13.754619372007094}, {"em": 8.268933539412673, "f1": 13.391789355184054}]}, "total": {"test_em": 8.335444796154349, "test_em_se": 0.43962695864607526, "test_f1": 13.747086547961835, "test_f1_se": 0.39786301843722016}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_7800", "results": {"raw": {"test": [{"em": 0.0, "f1": 3.5089118144061517}, {"em": 0.07751937984496124, "f1": 5.016029942530271}, {"em": 0.0, "f1": 3.8309141574364607}]}, "total": {"test_em": 0.025839793281653745, "test_em_se": 0.050645994832041345, "test_f1": 4.118618638124294, "test_f1_se": 0.8981361220395749}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_7800", "results": {"raw": {"test": [{"em": 0.0, "f1": 3.5089118144061517}, {"em": 0.07751937984496124, "f1": 5.016029942530271}, {"em": 0.0, "f1": 3.8309141574364607}]}, "total": {"test_em": 0.025839793281653745, "test_em_se": 0.050645994832041345, "test_f1": 4.118618638124294, "test_f1_se": 0.8981361220395749}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_7800", "results": {"raw": {"test": [{"em": 0.0, "f1": 3.5089118144061517}]}, "total": {"test_em": 0.0, "test_em_se": NaN, "test_f1": 3.5089118144061517, "test_f1_se": NaN}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_1300", "results": {"raw": {"test": [{"em": 4.415182029434547, "f1": 11.59790464343451}, {"em": 0.6976744186046512, "f1": 8.141858680746845}, {"em": 1.777434312210201, "f1": 8.371385590911304}]}, "total": {"test_em": 2.2967635867497997, "test_em_se": 2.164075187422901, "test_f1": 9.370382971697552, "test_f1_se": 2.186830785497224}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_2600", "results": {"raw": {"test": [{"em": 16.886134779240898, "f1": 23.050568049720535}, {"em": 18.6046511627907, "f1": 24.124974766146053}, {"em": 18.39258114374034, "f1": 24.34530463287618}]}, "total": {"test_em": 17.96112236192398, "test_em_se": 1.0602991032595825, "test_f1": 23.840282482914258, "test_f1_se": 0.783896257718189}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_1300", "results": {"raw": {"test": [{"em": 4.415182029434547, "f1": 11.59790464343451}, {"em": 0.6976744186046512, "f1": 8.141858680746845}, {"em": 1.777434312210201, "f1": 8.371385590911304}]}, "total": {"test_em": 2.2967635867497997, "test_em_se": 2.164075187422901, "test_f1": 9.370382971697552, "test_f1_se": 2.186830785497224}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_2600", "results": {"raw": {"test": [{"em": 16.886134779240898, "f1": 23.050568049720535}, {"em": 18.6046511627907, "f1": 24.124974766146053}, {"em": 18.39258114374034, "f1": 24.34530463287618}]}, "total": {"test_em": 17.96112236192398, "test_em_se": 1.0602991032595825, "test_f1": 23.840282482914258, "test_f1_se": 0.783896257718189}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_3900", "results": {"raw": {"test": [{"em": 0.6196746707978311, "f1": 6.430495892538802}, {"em": 1.627906976744186, "f1": 7.598106965741532}, {"em": 0.3091190108191654, "f1": 5.279799336200877}]}, "total": {"test_em": 0.8522335527870609, "test_em_se": 0.7802040495688798, "test_f1": 6.43613406482707, "test_f1_se": 1.311717660819032}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_1300", "results": {"raw": {"test": [{"em": 4.415182029434547, "f1": 11.59790464343451}, {"em": 0.6976744186046512, "f1": 8.141858680746845}, {"em": 1.777434312210201, "f1": 8.371385590911304}]}, "total": {"test_em": 2.2967635867497997, "test_em_se": 2.164075187422901, "test_f1": 9.370382971697552, "test_f1_se": 2.186830785497224}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_2600", "results": {"raw": {"test": [{"em": 16.886134779240898, "f1": 23.050568049720535}, {"em": 18.6046511627907, "f1": 24.124974766146053}, {"em": 18.39258114374034, "f1": 24.34530463287618}]}, "total": {"test_em": 17.96112236192398, "test_em_se": 1.0602991032595825, "test_f1": 23.840282482914258, "test_f1_se": 0.783896257718189}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_5200", "results": {"raw": {"test": [{"em": 0.69713400464756, "f1": 5.25658538239424}, {"em": 1.317829457364341, "f1": 7.618729364054122}, {"em": 1.1591962905718702, "f1": 6.719507033911071}]}, "total": {"test_em": 1.058053250861257, "test_em_se": 0.3649113435551277, "test_f1": 6.53160726011981, "test_f1_se": 1.3491344998093988}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_6500", "results": {"raw": {"test": [{"em": 0.0, "f1": 3.601785383542276}, {"em": 0.07751937984496124, "f1": 4.190543110863382}, {"em": 0.0, "f1": 3.675359378038974}]}, "total": {"test_em": 0.025839793281653745, "test_em_se": 0.050645994832041345, "test_f1": 3.8225626241482114, "test_f1_se": 0.3630156210394486}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_9100", "results": {"raw": {"test": [{"em": 0.0, "f1": 2.277752180741008}, {"em": 0.15503875968992248, "f1": 2.804295814063316}, {"em": 0.0, "f1": 2.6451745285122095}]}, "total": {"test_em": 0.05167958656330749, "test_em_se": 0.10129198966408269, "test_f1": 2.5757408411055107, "test_f1_se": 0.3055920705837912}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_10400", "results": {"raw": {"test": [{"em": 1.0844306738962044, "f1": 4.667945255421082}, {"em": 1.0852713178294573, "f1": 5.500937958250401}, {"em": 0.46367851622874806, "f1": 3.7421323430646067}]}, "total": {"test_em": 0.8777935026514698, "test_em_se": 0.40583296542053227, "test_f1": 4.637005185578697, "test_f1_se": 0.9955997859190142}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_11700", "results": {"raw": {"test": [{"em": 0.15491866769945778, "f1": 3.9328589066347748}, {"em": 0.07751937984496124, "f1": 3.833857998675171}, {"em": 0.0, "f1": 4.288739385406335}]}, "total": {"test_em": 0.077479349181473, "test_em_se": 0.08765349658767232, "test_f1": 4.01848543023876, "test_f1_se": 0.270707612573117}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_13000", "results": {"raw": {"test": [{"em": 0.07745933384972889, "f1": 4.100454225419051}, {"em": 0.0, "f1": 4.418886104499059}, {"em": 0.0, "f1": 3.9149160572332953}]}, "total": {"test_em": 0.02581977794990963, "test_em_se": 0.05060676478182288, "test_f1": 4.144752129050468, "test_f1_se": 0.2884335634505966}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/final_model", "results": {"raw": {"test": [{"em": 0.07745933384972889, "f1": 3.8713917114319876}, {"em": 0.07751937984496124, "f1": 4.061850098953285}, {"em": 0.07727975270479134, "f1": 3.942117766621321}]}, "total": {"test_em": 0.07741948879982716, "test_em_se": 0.00014109283222840812, "test_f1": 3.958453192335531, "test_f1_se": 0.10894458579210015}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_1300", "results": {"raw": {"test": [{"em": 4.415182029434547, "f1": 11.59790464343451}, {"em": 0.6976744186046512, "f1": 8.141858680746845}, {"em": 1.777434312210201, "f1": 8.371385590911304}]}, "total": {"test_em": 2.2967635867497997, "test_em_se": 2.164075187422901, "test_f1": 9.370382971697552, "test_f1_se": 2.186830785497224}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_2600", "results": {"raw": {"test": [{"em": 16.886134779240898, "f1": 23.050568049720535}, {"em": 18.6046511627907, "f1": 24.124974766146053}, {"em": 18.39258114374034, "f1": 24.34530463287618}]}, "total": {"test_em": 17.96112236192398, "test_em_se": 1.0602991032595825, "test_f1": 23.840282482914258, "test_f1_se": 0.783896257718189}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_1300", "results": {"raw": {"test": [{"em": 4.415182029434547, "f1": 11.59790464343451}, {"em": 0.6976744186046512, "f1": 8.141858680746845}, {"em": 1.777434312210201, "f1": 8.371385590911304}]}, "total": {"test_em": 2.2967635867497997, "test_em_se": 2.164075187422901, "test_f1": 9.370382971697552, "test_f1_se": 2.186830785497224}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_2600", "results": {"raw": {"test": [{"em": 16.886134779240898, "f1": 23.050568049720535}, {"em": 18.6046511627907, "f1": 24.124974766146053}, {"em": 18.39258114374034, "f1": 24.34530463287618}]}, "total": {"test_em": 17.96112236192398, "test_em_se": 1.0602991032595825, "test_f1": 23.840282482914258, "test_f1_se": 0.783896257718189}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_3900", "results": {"raw": {"test": [{"em": 0.6196746707978311, "f1": 6.430495892538801}, {"em": 1.627906976744186, "f1": 7.598106965741526}, {"em": 0.3091190108191654, "f1": 5.279799336200871}]}, "total": {"test_em": 0.8522335527870609, "test_em_se": 0.7802040495688798, "test_f1": 6.4361340648270655, "test_f1_se": 1.3117176608190322}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_5200", "results": {"raw": {"test": [{"em": 0.69713400464756, "f1": 5.25658538239424}, {"em": 1.317829457364341, "f1": 7.618729364054122}, {"em": 1.1591962905718702, "f1": 6.719507033911071}]}, "total": {"test_em": 1.058053250861257, "test_em_se": 0.3649113435551277, "test_f1": 6.53160726011981, "test_f1_se": 1.3491344998093988}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "models/instruction/20241129204056/step_6500", "results": {"raw": {"test": [{"em": 0.0, "f1": 3.601785383542277}, {"em": 0.07751937984496124, "f1": 4.190543110863386}, {"em": 0.0, "f1": 3.675359378038974}]}, "total": {"test_em": 0.025839793281653745, "test_em_se": 0.050645994832041345, "test_f1": 3.8225626241482122, "test_f1_se": 0.36301562103945056}}, "num_model_parameters": -1, "max_sequence_length": 32768, "vocabulary_size": 151668, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "Qwen/Qwen2.5-0.5B", "results": {"raw": {"test": [{"em": 36.17350890782339, "f1": 41.50939678881996}, {"em": 34.031007751937985, "f1": 39.52611588231504}, {"em": 34.31221020092736, "f1": 39.68336808356357}]}, "total": {"test_em": 34.83890895356291, "test_em_se": 1.31754988674485, "test_f1": 40.23962691823286, "test_f1_se": 1.2475512687366015}}, "num_model_parameters": 494032768, "max_sequence_length": 32768, "vocabulary_size": 151936, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "Qwen/Qwen2.5-0.5B", "results": {"raw": {"test": [{"em": 36.17350890782339, "f1": 41.50939678881995}, {"em": 34.031007751937985, "f1": 39.526115882315054}, {"em": 34.31221020092736, "f1": 39.683368083563565}, {"em": 34.190031152647975, "f1": 40.18702287411961}, {"em": 37.99227799227799, "f1": 43.416785059642216}, {"em": 36.160370084811106, "f1": 41.873242314371055}, {"em": 34.47228549734245, "f1": 41.32667003965411}, {"em": 35.453840186190845, "f1": 40.518942494320015}, {"em": 30.980392156862745, "f1": 35.66524446974276}, {"em": 36.56832298136646, "f1": 42.60689618403358}]}, "total": {"test_em": 35.033424691218826, "test_em_se": 1.183877594721725, "test_f1": 40.63136841905819, "test_f1_se": 1.3301494778631475}}, "num_model_parameters": 494032768, "max_sequence_length": 32768, "vocabulary_size": 151936, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "Qwen/Qwen2.5-0.5B", "results": {"raw": {"test": [{"bertscore": 0.5406629760400392, "rouge_l": 0.09334468736155466}, {"bertscore": 0.5015091803506948, "rouge_l": 0.07766623121955829}, {"bertscore": 0.5552412762481254, "rouge_l": 0.0713480917249821}, {"bertscore": 0.5124500314923353, "rouge_l": 0.09214877471437308}, {"bertscore": 0.5578031182521954, "rouge_l": 0.10417832381478254}, {"bertscore": 0.5310678549867589, "rouge_l": 0.09511579674820383}, {"bertscore": 0.5574570146927726, "rouge_l": 0.1013158147907528}, {"bertscore": 0.4504405618281453, "rouge_l": 0.0733585919884524}, {"bertscore": 0.5233130506894668, "rouge_l": 0.09647599013895794}, {"bertscore": 0.5035603596552392, "rouge_l": 0.08505153300272512}]}, "total": {"test_bertscore": 52.33505424235773, "test_bertscore_se": 2.0691090238317513, "test_rouge_l": 8.900038355043428, "test_rouge_l_se": 0.7170116828019738}}, "num_model_parameters": 494032768, "max_sequence_length": 32768, "vocabulary_size": 151936, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "Qwen/Qwen2.5-0.5B", "results": {"raw": {"test": [{"mcc": 0.0, "macro_f1": 0.33159268929503916}, {"mcc": 0.0, "macro_f1": 0.3382875605815832}, {"mcc": 0.0, "macro_f1": 0.3254281949934124}, {"mcc": 0.0, "macro_f1": 0.33202870189171557}, {"mcc": 0.0, "macro_f1": 0.33484897694056515}, {"mcc": 0.0, "macro_f1": 0.3311561071195297}, {"mcc": 0.0, "macro_f1": 0.3372168284789644}, {"mcc": 0.0, "macro_f1": 0.33006215243702974}, {"mcc": 0.0, "macro_f1": 0.3302812295618051}, {"mcc": 0.0, "macro_f1": 0.3342002600780234}]}, "total": {"test_mcc": 0.0, "test_mcc_se": 0.0, "test_macro_f1": 33.25102701377667, "test_macro_f1_se": 0.23363377434653787}}, "num_model_parameters": 494032768, "max_sequence_length": 32768, "vocabulary_size": 151936, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.2.0"}